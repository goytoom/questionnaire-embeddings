{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0887e99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data frames and calculations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# file structure\n",
    "from pathlib import Path\n",
    "\n",
    "# cross-validation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#custom functions\n",
    "from functions import getResponses, chooseData, chooseEmb, getEmbeddings, getData, predModel, corrUserBased #general auxiliary function\n",
    "\n",
    "#plots\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"An input array is constant; the correlation coefficient is not defined.\")\n",
    "\n",
    "#set random state for stochastic processes\n",
    "randState = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e4deb3",
   "metadata": {},
   "source": [
    "## Choose data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "946724fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d = \"hsq\" #BIG5. 16PF, RIASEC, HSQ\n",
    "h_path = \"../human_studies/\" + d.upper() + \"/\" + d.lower() \n",
    "\n",
    "# create folder for saving human raters data (files to run create survey)\n",
    "Path(\"../human_studies/\" + d.upper()).mkdir(parents=True, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac86b4a-5c4b-4652-a04f-d471567fbcbd",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52123225-6453-465c-9c85-485ee14817f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Choose Data Set:\n",
    "R = 2           #1: reversed-coded, #2: nonReversed-coded\n",
    "\n",
    "#best Model (KnnRegression k=5, no reverse-coding):\n",
    "m = 4          #0: Ridge, #1: RidgeClass, #2:KNN, #3: Kernel SVM (RBF), #4: KNN regression\n",
    "par = 5\n",
    "e = 'sentencebert'\n",
    "model, modelName = predModel(m,par) \n",
    "\n",
    "#load path and necessary variables:\n",
    "folder, data = chooseData(d)        # BIG5, IPIP (all items), IPIP2 (only assigned items), RIASEC, HEXACO, 16PF\n",
    "embeddings, save = chooseEmb(e)     #USE, BERT, SENTENCEBERT\n",
    "responses, savePath, items, _ = getResponses(folder, data, R) #1: Reversed, #2: nonReversed\n",
    "responses = responses.astype(float) #get observed responses as floats\n",
    "X, X_stand, X_pca_stand = getEmbeddings(folder, data, embeddings, responses)\n",
    "\n",
    "#get embeddings name:\n",
    "embName = embeddings.split(\"_\")[2].split(\".\")[0]\n",
    "\n",
    "# import required data and labels:\n",
    "data_q, constructs_list, list_par, constrAssigned = getData(1, responses, X_pca_stand, folder, data)\n",
    "\n",
    "# get predicted responses of chosen model:\n",
    "total_preds = pd.read_csv(savePath + modelName + \"_\" + str(par) + \"_\" + embName + \"_responses.csv\", index_col=0)\n",
    "total_preds.index = total_preds.index.map(str)\n",
    "total_preds = total_preds.astype(float)\n",
    "\n",
    "#get performance:\n",
    "corr, means = corrUserBased(total_preds, responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533ee002-4045-4e0d-8d9f-8ef23130141e",
   "metadata": {},
   "source": [
    "Find Target Participants (0th-100th percentiles in 60 steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5103fdd-3f37-403d-82fa-fb8da3043d62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "error = 0.0001 # for rounding errors\n",
    "\n",
    "# sort targets based on model's predictive performance\n",
    "sorted = corr.sort_values(\"Correlation\")\n",
    "S = sorted.Correlation\n",
    "P = sorted['p-value']\n",
    "percentage_rank = S.rank(method=\"max\", pct=True)\n",
    "sorted[\"percentile\"] = percentage_rank\n",
    "\n",
    "# set number of targets and corresponding predictive accuracy percentiles (equidistant)\n",
    "nr_targets = 60\n",
    "percentiles = np.linspace(0, 1, nr_targets)\n",
    "\n",
    "#print details (yes/no)\n",
    "verbose = False\n",
    "\n",
    "ids = []\n",
    "# iterate over sorted list and save all targets (chosen based on accuracy percentiles)\n",
    "for q in percentiles:\n",
    "  idx = S.index[percentage_rank >= q-error]\n",
    "  ids.append(idx[0])\n",
    "  \n",
    "  #print predictive accuracy for each target\n",
    "  if verbose==1:\n",
    "      print(\"Rank: \" + str(round(percentage_rank[idx[0]], 3)))\n",
    "      print('Target ID: ' + idx[0])\n",
    "      print(\"Correlation: \" + str(S[idx[0]]))\n",
    "      print(\"p-value: \" + str(P[idx[0]]))\n",
    "      CI = r_confidence_interval(S[idx[0]], 0.95, responses.shape[1])\n",
    "      print(\"CI: [\" + str(CI[0]) + \", \" + str(CI[1]) + \"]\")\n",
    "      print(\"\\n\")\n",
    "\n",
    "# Save Target information (performance) in dataframe\n",
    "targets_ranked = sorted.loc[ids]\n",
    "targets_ranked.insert(0, \"target_nr\", list(range(2,62)))\n",
    "targets_responses = responses.loc[ids]\n",
    "targets_ranked_nr = [\"Field \" + str(i) for i in range(2,62)]\n",
    "\n",
    "# Merge target performance information and item responses\n",
    "targets_data = pd.merge(targets_ranked, targets_responses, left_index=True, right_index=True).drop(labels = [\"L1 Loss\"], axis = 1)\n",
    "# format target responses (rows -- questions, columns -- targets)\n",
    "targets_data_processed = targets_data.drop([\"target_nr\", \"Correlation\", \"p-value\", \"percentile\"], axis=1).T\n",
    "targets_data_processed.columns = list(targets_ranked_nr)\n",
    "targets_data_processed.index.name = \"question-id\"\n",
    "\n",
    "# Save target information\n",
    "targets_data.to_csv(h_path + \"_targets_data.csv\") # save to files\n",
    "\n",
    "### in case something breaks, load backUp:\n",
    "# targets_data = pd.read_csv(\"../human_studies/BackUps\" + \"/\" + d.lower() + \"_full_targets_questions.csv\", index_col = 0) # load backUp to replicate study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccc6f92-cf5e-438f-86ad-f5f6db722b6d",
   "metadata": {},
   "source": [
    "Extract train/test-folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "026b3957-9829-4169-b3fd-7d7210198f06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, random_state=randState, shuffle=True)\n",
    "questions = list(kf.split(data_q))\n",
    "questions = [(x+1,y+1) for x,y in questions]\n",
    "question_texts = pd.read_csv(\"../embeddings/\" + d.upper() + \"/\" + d.lower() + \"_questions_text.csv\", index_col=0) #get question texts\n",
    "\n",
    "if d.upper() == \"BIG5\":\n",
    "    colname = \"grammartical_item\"\n",
    "else:\n",
    "    colname = \"item\"\n",
    "# merge response data (sorted targets) with question texts for qualtrics import (lopp & merge)\n",
    "target_data_full = pd.merge(question_texts[colname], targets_data_processed, on=\"question-id\")\n",
    "target_data_full = target_data_full.rename(columns={colname: 'full_item'})\n",
    "target_data_full.full_item = target_data_full.full_item.str.capitalize()\n",
    "target_data_full.to_csv(h_path + \"_full_targets_questions.csv\")\n",
    "\n",
    "### in case something breaks, load a backup:\n",
    "# target_data_full = pd.read_csv(\"../human_studies/BackUps\" + \"/\" + d.lower() + \"_full_targets_questions.csv\", index_col = 0) \n",
    "\n",
    "folds = []\n",
    "for nr, fold in enumerate(questions):\n",
    "    train = questions[nr][0]\n",
    "    test = questions[nr][1]\n",
    "    folds.append([nr+1, train, test])\n",
    "    target_data_fold_train = target_data_full.iloc[train-1]\n",
    "    target_data_fold_test  = target_data_full.iloc[test-1]\n",
    "    target_data_fold_train.to_csv(h_path + \"_train_fold_\" + str(nr+1) + \".csv\")\n",
    "    target_data_fold_test.to_csv(h_path + \"_test_fold_\" + str(nr+1) + \".csv\")\n",
    "    \n",
    "df_folds = pd.DataFrame(folds, columns=[\"fold_nr\", \"train_items\", \"test_items\"])\n",
    "df_folds.to_csv(h_path + \"_question_folds.csv\", index=False) # save to files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1504c80-bafe-40a8-b75f-5c3409e2f954",
   "metadata": {},
   "source": [
    "Show the test folds (for survey creation in qualtrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a58987b-1f6a-4e61-8dfd-b6de18324b9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 7],\n",
       " [2, 7],\n",
       " [3, 4],\n",
       " [4, 6],\n",
       " [5, 8],\n",
       " [6, 8],\n",
       " [7, 6],\n",
       " [8, 1],\n",
       " [9, 2],\n",
       " [10, 10],\n",
       " [11, 4],\n",
       " [12, 7],\n",
       " [13, 7],\n",
       " [14, 5],\n",
       " [15, 7],\n",
       " [16, 6],\n",
       " [17, 3],\n",
       " [18, 8],\n",
       " [19, 4],\n",
       " [20, 3],\n",
       " [21, 6],\n",
       " [22, 10],\n",
       " [23, 3],\n",
       " [24, 6],\n",
       " [25, 2],\n",
       " [26, 9],\n",
       " [27, 1],\n",
       " [28, 3],\n",
       " [29, 9],\n",
       " [30, 9],\n",
       " [31, 4],\n",
       " [32, 9],\n",
       " [33, 9],\n",
       " [34, 2],\n",
       " [35, 8],\n",
       " [36, 7],\n",
       " [37, 10],\n",
       " [38, 1],\n",
       " [39, 8],\n",
       " [40, 10],\n",
       " [41, 3],\n",
       " [42, 7],\n",
       " [43, 8],\n",
       " [44, 4],\n",
       " [45, 1],\n",
       " [46, 2],\n",
       " [47, 7],\n",
       " [48, 10],\n",
       " [49, 5],\n",
       " [50, 5],\n",
       " [51, 5],\n",
       " [52, 2],\n",
       " [53, 6],\n",
       " [54, 8],\n",
       " [55, 1],\n",
       " [56, 2],\n",
       " [57, 3],\n",
       " [58, 7],\n",
       " [59, 10],\n",
       " [60, 4],\n",
       " [61, 3],\n",
       " [62, 4],\n",
       " [63, 1],\n",
       " [64, 2],\n",
       " [65, 5],\n",
       " [66, 7],\n",
       " [67, 3],\n",
       " [68, 10],\n",
       " [69, 6],\n",
       " [70, 5],\n",
       " [71, 10],\n",
       " [72, 4],\n",
       " [73, 9],\n",
       " [74, 1],\n",
       " [75, 4],\n",
       " [76, 6],\n",
       " [77, 6],\n",
       " [78, 9],\n",
       " [79, 6],\n",
       " [80, 9],\n",
       " [81, 1],\n",
       " [82, 10],\n",
       " [83, 9],\n",
       " [84, 5],\n",
       " [85, 6],\n",
       " [86, 4],\n",
       " [87, 4],\n",
       " [88, 10],\n",
       " [89, 10],\n",
       " [90, 2],\n",
       " [91, 1],\n",
       " [92, 7],\n",
       " [93, 2],\n",
       " [94, 2],\n",
       " [95, 1],\n",
       " [96, 6],\n",
       " [97, 4],\n",
       " [98, 4],\n",
       " [99, 5],\n",
       " [100, 9],\n",
       " [101, 1],\n",
       " [102, 2],\n",
       " [103, 7],\n",
       " [104, 10],\n",
       " [105, 8],\n",
       " [106, 8],\n",
       " [107, 4],\n",
       " [108, 3],\n",
       " [109, 3],\n",
       " [110, 5],\n",
       " [111, 4],\n",
       " [112, 5],\n",
       " [113, 5],\n",
       " [114, 3],\n",
       " [115, 9],\n",
       " [116, 9],\n",
       " [117, 8],\n",
       " [118, 10],\n",
       " [119, 3],\n",
       " [120, 1],\n",
       " [121, 6],\n",
       " [122, 3],\n",
       " [123, 6],\n",
       " [124, 5],\n",
       " [125, 2],\n",
       " [126, 1],\n",
       " [127, 7],\n",
       " [128, 9],\n",
       " [129, 8],\n",
       " [130, 8],\n",
       " [131, 5],\n",
       " [132, 1],\n",
       " [133, 3],\n",
       " [134, 8],\n",
       " [135, 3],\n",
       " [136, 5],\n",
       " [137, 1],\n",
       " [138, 9],\n",
       " [139, 2],\n",
       " [140, 7],\n",
       " [141, 10],\n",
       " [142, 8],\n",
       " [143, 2],\n",
       " [144, 9],\n",
       " [145, 3],\n",
       " [146, 4],\n",
       " [147, 2],\n",
       " [148, 5],\n",
       " [149, 3],\n",
       " [150, 1],\n",
       " [151, 2],\n",
       " [152, 8],\n",
       " [153, 7],\n",
       " [154, 10],\n",
       " [155, 2],\n",
       " [156, 10],\n",
       " [157, 8],\n",
       " [158, 5],\n",
       " [159, 9],\n",
       " [160, 7],\n",
       " [161, 1],\n",
       " [162, 6],\n",
       " [163, 6]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_overview = []\n",
    "for i in range(question_texts.shape[0]):\n",
    "    foldNr = df_folds.fold_nr[df_folds.test_items.apply(lambda x: i+1 in x)].iloc[0]\n",
    "    fold_overview.append([i+1, foldNr])\n",
    "    \n",
    "fold_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1f6b78d-a024-4d7b-ac3a-f64b65bf11ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d = \"hsq\" #BIG5. 16PF, RIASEC, HSQ\n",
    "h_path = \"../human_studies/\" + d.upper() + \"/\" + d.lower() \n",
    "\n",
    "# create folder for saving human raters data (files to run create survey)\n",
    "Path(\"../human_studies/\" + d.upper()).mkdir(parents=True, exist_ok = True)\n",
    "\n",
    "#Choose Data Set:\n",
    "R = 2           #1: reversed-coded, #2: nonReversed-coded\n",
    "\n",
    "#best Model (KnnRegression k=5, no reverse-coding):\n",
    "m = 4          #0: Ridge, #1: RidgeClass, #2:KNN, #3: Kernel SVM (RBF), #4: KNN regression\n",
    "par = 5\n",
    "e = 'sentencebert'\n",
    "model, modelName = predModel(m,par) \n",
    "\n",
    "#load path and necessary variables:\n",
    "folder, data = chooseData(d)        # BIG5, IPIP (all items), IPIP2 (only assigned items), RIASEC, HEXACO, 16PF\n",
    "embeddings, save = chooseEmb(e)     #USE, BERT, SENTENCEBERT\n",
    "responses, savePath, items, _ = getResponses(folder, data, R) #1: Reversed, #2: nonReversed\n",
    "responses = responses.astype(float) #get observed responses as floats\n",
    "X, X_stand, X_pca_stand = getEmbeddings(folder, data, embeddings, responses)\n",
    "\n",
    "#get embeddings name:\n",
    "embName = embeddings.split(\"_\")[2].split(\".\")[0]\n",
    "\n",
    "# import required data and labels:\n",
    "data_q, constructs_list, list_par, constrAssigned = getData(1, responses, X_pca_stand, folder, data)\n",
    "\n",
    "# get predicted responses of chosen model:\n",
    "total_preds = pd.read_csv(savePath + modelName + \"_\" + str(par) + \"_\" + embName + \"_responses.csv\", index_col=0)\n",
    "total_preds.index = total_preds.index.map(str)\n",
    "total_preds = total_preds.astype(float)\n",
    "\n",
    "#get performance:\n",
    "corr, means = corrUserBased(total_preds, responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "68b3048a-4f09-4352-840e-da257366253c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "error = 0.0001 # for rounding errors\n",
    "\n",
    "# sort targets based on model's predictive performance\n",
    "sorted = corr.sort_values(\"Correlation\")\n",
    "S = sorted.Correlation\n",
    "P = sorted['p-value']\n",
    "percentage_rank = S.rank(method=\"max\", pct=True)\n",
    "sorted[\"percentile\"] = percentage_rank\n",
    "\n",
    "# set number of targets and corresponding predictive accuracy percentiles (equidistant)\n",
    "nr_targets = 60\n",
    "percentiles = np.linspace(0, 1, nr_targets)\n",
    "\n",
    "#print details (yes/no)\n",
    "verbose = False\n",
    "\n",
    "ids = []\n",
    "# iterate over sorted list and save all targets (chosen based on accuracy percentiles)\n",
    "for q in percentiles:\n",
    "  idx = S.index[percentage_rank >= q-error]\n",
    "  ids.append(idx[0])\n",
    "  \n",
    "  #print predictive accuracy for each target\n",
    "  if verbose==1:\n",
    "      print(\"Rank: \" + str(round(percentage_rank[idx[0]], 3)))\n",
    "      print('Target ID: ' + idx[0])\n",
    "      print(\"Correlation: \" + str(S[idx[0]]))\n",
    "      print(\"p-value: \" + str(P[idx[0]]))\n",
    "      CI = r_confidence_interval(S[idx[0]], 0.95, responses.shape[1])\n",
    "      print(\"CI: [\" + str(CI[0]) + \", \" + str(CI[1]) + \"]\")\n",
    "      print(\"\\n\")\n",
    "\n",
    "# Save Target information (performance) in dataframe\n",
    "targets_ranked = sorted.loc[ids]\n",
    "targets_ranked.insert(0, \"target_nr\", list(range(2,62)))\n",
    "targets_responses = responses.loc[ids]\n",
    "targets_ranked_nr = [\"Field \" + str(i) for i in range(2,62)]\n",
    "\n",
    "# Merge target performance information and item responses\n",
    "targets_data = pd.merge(targets_ranked, targets_responses, left_index=True, right_index=True).drop(labels = [\"L1 Loss\"], axis = 1)\n",
    "# format target responses (rows -- questions, columns -- targets)\n",
    "targets_data_processed = targets_data.drop([\"target_nr\", \"Correlation\", \"p-value\", \"percentile\"], axis=1).T\n",
    "targets_data_processed.columns = list(targets_ranked_nr)\n",
    "targets_data_processed.index.name = \"question-id\"\n",
    "\n",
    "# Save target information\n",
    "# targets_data.to_csv(h_path + \"_targets_data.csv\") # save to files\n",
    "\n",
    "### in case something breaks, load backUp:\n",
    "# targets_data = pd.read_csv(\"../human_studies/BackUps\" + \"/\" + d.lower() + \"_full_targets_questions.csv\", index_col = 0) # load backUp to replicate study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4890bde0-8253-46f6-8ef3-c7223940a278",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=10, random_state=randState, shuffle=True)\n",
    "questions = list(kf.split(data_q))\n",
    "questions = [(x+1,y+1) for x,y in questions]\n",
    "question_texts = pd.read_csv(\"../embeddings/\" + d.upper() + \"/\" + d.lower() + \"_questions_text.csv\", index_col=0) #get question texts\n",
    "\n",
    "if d.upper() == \"BIG5\":\n",
    "    colname = \"grammartical_item\"\n",
    "else:\n",
    "    colname = \"item\"\n",
    "# merge response data (sorted targets) with question texts for qualtrics import (lopp & merge)\n",
    "target_data_full = pd.merge(question_texts[colname], targets_data_processed, on=\"question-id\")\n",
    "target_data_full = target_data_full.rename(columns={colname: 'full_item'})\n",
    "target_data_full.full_item = target_data_full.full_item.str.capitalize()\n",
    "# target_data_full.to_csv(h_path + \"_full_targets_questions.csv\")\n",
    "\n",
    "### in case something breaks, load a backup:\n",
    "# target_data_full = pd.read_csv(\"../human_studies/BackUps\" + \"/\" + d.lower() + \"_full_targets_questions.csv\", index_col = 0) \n",
    "\n",
    "folds = []\n",
    "for nr, fold in enumerate(questions):\n",
    "    train = questions[nr][0]\n",
    "    test = questions[nr][1]\n",
    "    folds.append([nr+1, train, test])\n",
    "    target_data_fold_train = target_data_full.iloc[train-1]\n",
    "    target_data_fold_test  = target_data_full.iloc[test-1]\n",
    "    # target_data_fold_train.to_csv(h_path + \"_train_fold_\" + str(nr+1) + \".csv\")\n",
    "    # target_data_fold_test.to_csv(h_path + \"_test_fold_\" + str(nr+1) + \".csv\")\n",
    "    \n",
    "df_folds = pd.DataFrame(folds, columns=[\"fold_nr\", \"train_items\", \"test_items\"])\n",
    "# df_folds.to_csv(h_path + \"_question_folds.csv\", index=False) # save to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa30670e-4613-48c0-a158-3ab66d40e963",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 9],\n",
       " [2, 6],\n",
       " [3, 1],\n",
       " [4, 9],\n",
       " [5, 7],\n",
       " [6, 4],\n",
       " [7, 6],\n",
       " [8, 8],\n",
       " [9, 4],\n",
       " [10, 8],\n",
       " [11, 1],\n",
       " [12, 1],\n",
       " [13, 10],\n",
       " [14, 3],\n",
       " [15, 2],\n",
       " [16, 10],\n",
       " [17, 2],\n",
       " [18, 4],\n",
       " [19, 7],\n",
       " [20, 8],\n",
       " [21, 3],\n",
       " [22, 10],\n",
       " [23, 1],\n",
       " [24, 5],\n",
       " [25, 3],\n",
       " [26, 5],\n",
       " [27, 2],\n",
       " [28, 9],\n",
       " [29, 2],\n",
       " [30, 7],\n",
       " [31, 5],\n",
       " [32, 6]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_overview = []\n",
    "for i in range(question_texts.shape[0]):\n",
    "    foldNr = df_folds.fold_nr[df_folds.test_items.apply(lambda x: i+1 in x)].iloc[0]\n",
    "    fold_overview.append([i+1, foldNr])\n",
    "    \n",
    "fold_overview"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
